{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JmrGW1a4WXmJ"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r60ytkCGwpsB",
        "outputId": "8a6626c5-9685-46d7-c4f5-db0569300a68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'drive/My Drive/'\n",
        "train_lang = 'en'"
      ],
      "metadata": {
        "id": "bXtjnkCPwTug"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetSeq(Dataset):\n",
        "    def __init__(self, data_dir, train_lang='en'):\n",
        "\t#open file\n",
        "        with open(data_dir + train_lang + '.train', 'r') as f:\n",
        "            train = f.read().split('\\n\\n')\n",
        "\n",
        "        # delete extra tag markup\n",
        "        train = [x for x in train if not '_ ' in x]\n",
        "\t    #init vocabs of tokens for encoding {<str> token: <int> id}\n",
        "        self.target_vocab = {'<pad>': 0} # {p: 1, a: 2, r: 3, pu: 4}\n",
        "        self.word_vocab = {'<pad>': 0} # {cat: 1, sat: 2, on: 3, mat: 4, '.': 5}\n",
        "        self.char_vocab = {'<pad>': 0} # {c: 1, a: 2, t: 3, ' ': 4, s: 5}\n",
        "\t    \n",
        "        # Cat sat on mat. -> [1, 2, 3, 4, 5]\n",
        "        # p    a  r  p pu -> [1, 2, 3, 1, 4]\n",
        "        # chars  -> [1, 2, 3, 4, 5, 2, 3, 4]\n",
        "\n",
        "\t    #init encoded sequences lists (processed data)\n",
        "        self.encoded_sequences = []\n",
        "        self.encoded_targets = []\n",
        "        self.encoded_char_sequences = []\n",
        "        # n=1 because first value is padding\n",
        "        n_word = 1\n",
        "        n_target = 1\n",
        "        n_char = 1\n",
        "        for line in train:\n",
        "            sequence = []\n",
        "            target = []\n",
        "            chars = []\n",
        "            for item in line.split('\\n'):\n",
        "                if item != '':\n",
        "                    word, label = item.split(' ')\n",
        "\n",
        "                    if self.word_vocab.get(word) is None:\n",
        "                        self.word_vocab[word] = n_word\n",
        "                        n_word += 1\n",
        "                    if self.target_vocab.get(label) is None:\n",
        "                        self.target_vocab[label] = n_target\n",
        "                        n_target += 1\n",
        "                    for char in word:\n",
        "                        if self.char_vocab.get(char) is None:\n",
        "                            self.char_vocab[char] = n_char\n",
        "                            n_char += 1\n",
        "                    sequence.append(self.word_vocab[word])\n",
        "                    target.append(self.target_vocab[label])\n",
        "                    chars.append([self.char_vocab[char] for char in word])\n",
        "            self.encoded_sequences.append(sequence)\n",
        "            self.encoded_targets.append(target)\n",
        "            self.encoded_char_sequences.append(chars)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            'data': self.encoded_sequences[index], # [1, 2, 3, 4, 6] len=5\n",
        "            'char': self.encoded_char_sequences[index],# [[1,2,3], [4,5], [1,2], [2,6,5,4], []] len=5\n",
        "            'target': self.encoded_targets[index], # [1, 2, 3, 4, 6] len=5\n",
        "        }"
      ],
      "metadata": {
        "id": "jYfgR8vmW6pL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DatasetSeq(data_dir)"
      ],
      "metadata": {
        "id": "2xv0zQB-W6rF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QStYtQp1W6tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    data = []\n",
        "    target = []\n",
        "    for item in batch:\n",
        "        data.append(torch.as_tensor(item['data']))\n",
        "        target.append(torch.as_tensor(item['target']))\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    target = pad_sequence(target, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'data': data, 'target': target}"
      ],
      "metadata": {
        "id": "bk_WthJCW6vW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper params\n",
        "vocab_size = len(dataset.word_vocab) + 1\n",
        "n_classes = len(dataset.target_vocab) + 1\n",
        "n_chars = len(dataset.char_vocab) + 1\n",
        "#TODO try to use other model parameters\n",
        "emb_dim = 256\n",
        "hidden = 256\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "cuda_device = 0\n",
        "batch_size = 100\n",
        "device = f'cuda:{cuda_device}' if cuda_device != -1 else 'cpu'"
      ],
      "metadata": {
        "id": "lysRxrzrW6yZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "J98lgeXxGLHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        #TODO try to use other RNN archicetures, f.e. RNN and LSTM\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
        "        self.do = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        emb = self.word_emb(x) # B x T x Emb_dim\n",
        "        hidden, _ = self.rnn(emb) # B x T x Hid, B x 1 x Hid\n",
        "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "HkHoeIhVbUZH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRU(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
        "model.train()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "fNGqx6AwW6zf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    dataloader = DataLoader(dataset, \n",
        "                            batch_size, \n",
        "                            shuffle=True, \n",
        "                            collate_fn=collate_fn,\n",
        "                            drop_last = True,\n",
        "                            )\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        predict = model(batch['data'].to(device))\n",
        "        loss = loss_func(predict.view(-1, n_classes),\n",
        "                         batch['target'].to(device).view(-1), \n",
        "                         )\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if i % 100 == 0:\n",
        "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
        "    GRU_train_time = datetime.datetime.now() - start\n",
        "   \n",
        "    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')\n",
        "print (f'Время тренировки GRU = {GRU_train_time}')"
      ],
      "metadata": {
        "id": "54uJ5yGCW61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32428472-bb8b-456c-bab9-9fdc2514b623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0, loss: 3.101123809814453\n",
            "epoch: 0, step: 100, loss: 0.12137915194034576\n",
            "epoch: 0, step: 200, loss: 0.1846797913312912\n",
            "epoch: 1, step: 0, loss: 0.1391931176185608\n",
            "epoch: 1, step: 100, loss: 0.12494346499443054\n",
            "epoch: 1, step: 200, loss: 0.14689116179943085\n",
            "epoch: 2, step: 0, loss: 0.12273569405078888\n",
            "epoch: 2, step: 100, loss: 0.13538816571235657\n",
            "epoch: 2, step: 200, loss: 0.13304944336414337\n",
            "epoch: 3, step: 0, loss: 0.08025465905666351\n",
            "epoch: 3, step: 100, loss: 0.11756011098623276\n",
            "epoch: 3, step: 200, loss: 0.08388648182153702\n",
            "epoch: 4, step: 0, loss: 0.03305644914507866\n",
            "epoch: 4, step: 100, loss: 0.06252439320087433\n",
            "epoch: 4, step: 200, loss: 0.06028808280825615\n",
            "epoch: 5, step: 0, loss: 0.05325816944241524\n",
            "epoch: 5, step: 100, loss: 0.04236844927072525\n",
            "epoch: 5, step: 200, loss: 0.056960202753543854\n",
            "epoch: 6, step: 0, loss: 0.05091463401913643\n",
            "epoch: 6, step: 100, loss: 0.04334740340709686\n",
            "epoch: 6, step: 200, loss: 0.03664012253284454\n",
            "epoch: 7, step: 0, loss: 0.03905167058110237\n",
            "epoch: 7, step: 100, loss: 0.038605786859989166\n",
            "epoch: 7, step: 200, loss: 0.05106775462627411\n",
            "epoch: 8, step: 0, loss: 0.04194408655166626\n",
            "epoch: 8, step: 100, loss: 0.02534186653792858\n",
            "epoch: 8, step: 200, loss: 0.03637184202671051\n",
            "epoch: 9, step: 0, loss: 0.024789301678538322\n",
            "epoch: 9, step: 100, loss: 0.022606350481510162\n",
            "epoch: 9, step: 200, loss: 0.02885483019053936\n",
            "Время тренировки GRU = 0:00:25.170961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'I am doing my homework right now'\n",
        "words = phrase.split(' ')\n",
        "tokens = [dataset.word_vocab[w] for w in words]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predict = model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
        "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
        "    GRU_inference_time = datetime.datetime.now() - start\n",
        "\n",
        "target_labels = list(dataset.target_vocab.keys())\n",
        "print([target_labels[l] for l in labels])\n",
        "print (f'Время инференса GRU = {GRU_inference_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO54_DW7u1oJ",
        "outputId": "a6c6fb50-42b2-4118-f37b-d5dcae9cd2f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV', 'ADV']\n",
            "Время инференса GRU = 0:00:00.002574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XI5_3F3s2uoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "5LrHKFGvGYgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
        "        self.do = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        emb = self.word_emb(x) # B x T x Emb_dim\n",
        "        hidden, _ = self.rnn(emb) # B x T x Hid, B x 1 x Hid\n",
        "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "0B17byGMFvc6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
        "model.train()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "3XasEe6zFw4y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    dataloader = DataLoader(dataset, \n",
        "                            batch_size, \n",
        "                            shuffle=True, \n",
        "                            collate_fn=collate_fn,\n",
        "                            drop_last = True,\n",
        "                            )\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        predict = model(batch['data'].to(device))\n",
        "        loss = loss_func(predict.view(-1, n_classes),\n",
        "                         batch['target'].to(device).view(-1), \n",
        "                         )\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if i % 100 == 0:\n",
        "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
        "    LSTM_train_time = datetime.datetime.now() - start\n",
        "   \n",
        "    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')\n",
        "print (f'Время тренировки LSTM = {LSTM_train_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBHUi-7iFxN9",
        "outputId": "8a3f3ae0-38f7-4f56-d04c-6264dbc36366"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0, loss: 2.9216668605804443\n",
            "epoch: 0, step: 100, loss: 0.26620611548423767\n",
            "epoch: 0, step: 200, loss: 0.24024461209774017\n",
            "epoch: 1, step: 0, loss: 0.19296017289161682\n",
            "epoch: 1, step: 100, loss: 0.17230208218097687\n",
            "epoch: 1, step: 200, loss: 0.0988345742225647\n",
            "epoch: 2, step: 0, loss: 0.20711670815944672\n",
            "epoch: 2, step: 100, loss: 0.14550676941871643\n",
            "epoch: 2, step: 200, loss: 0.10573887079954147\n",
            "epoch: 3, step: 0, loss: 0.09940575063228607\n",
            "epoch: 3, step: 100, loss: 0.09124055504798889\n",
            "epoch: 3, step: 200, loss: 0.0925726443529129\n",
            "epoch: 4, step: 0, loss: 0.06963233649730682\n",
            "epoch: 4, step: 100, loss: 0.08488699048757553\n",
            "epoch: 4, step: 200, loss: 0.05616848170757294\n",
            "epoch: 5, step: 0, loss: 0.06587868183851242\n",
            "epoch: 5, step: 100, loss: 0.05797233805060387\n",
            "epoch: 5, step: 200, loss: 0.05371752753853798\n",
            "epoch: 6, step: 0, loss: 0.057022180408239365\n",
            "epoch: 6, step: 100, loss: 0.05939161032438278\n",
            "epoch: 6, step: 200, loss: 0.04490207880735397\n",
            "epoch: 7, step: 0, loss: 0.03901951014995575\n",
            "epoch: 7, step: 100, loss: 0.026559654623270035\n",
            "epoch: 7, step: 200, loss: 0.03293541073799133\n",
            "epoch: 8, step: 0, loss: 0.02713264524936676\n",
            "epoch: 8, step: 100, loss: 0.039351724088191986\n",
            "epoch: 8, step: 200, loss: 0.04189769923686981\n",
            "epoch: 9, step: 0, loss: 0.03273094445466995\n",
            "epoch: 9, step: 100, loss: 0.03279920294880867\n",
            "epoch: 9, step: 200, loss: 0.02971205860376358\n",
            "Время тренировки LSTM = 0:02:21.649426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'I am doing my homework right now'\n",
        "words = phrase.split(' ')\n",
        "tokens = [dataset.word_vocab[w] for w in words]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predict = model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
        "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
        "    LSTM_inference_time = datetime.datetime.now() - start\n",
        "\n",
        "target_labels = list(dataset.target_vocab.keys())\n",
        "print([target_labels[l] for l in labels])\n",
        "print (f'Время инференса LSTM = {GRU_inference_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V157mlW7Fxf9",
        "outputId": "dcc34fc3-ad9b-4772-d85e-88d7d4a2a521"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV', 'ADV']\n",
            "Время инференса LSTM = 0:00:00.002180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTUDRjL1Fxx3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN"
      ],
      "metadata": {
        "id": "r2iDIBfxM_Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
        "        self.do = nn.Dropout(0.1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        emb = self.word_emb(x) # B x T x Emb_dim\n",
        "        hidden, _ = self.rnn(emb) # B x T x Hid, B x 1 x Hid\n",
        "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "MKBpXsGlFyBk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(vocab_size, emb_dim, hidden, n_classes).to(device)\n",
        "model.train()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "td8yTBQtNGXH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    dataloader = DataLoader(dataset, \n",
        "                            batch_size, \n",
        "                            shuffle=True, \n",
        "                            collate_fn=collate_fn,\n",
        "                            drop_last = True,\n",
        "                            )\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        predict = model(batch['data'].to(device))\n",
        "        loss = loss_func(predict.view(-1, n_classes),\n",
        "                         batch['target'].to(device).view(-1), \n",
        "                         )\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if i % 100 == 0:\n",
        "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
        "    RNN_train_time = datetime.datetime.now() - start\n",
        "   \n",
        "    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')\n",
        "print (f'Время тренировки LSTM = {RNN_train_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY-o1nJZNJgT",
        "outputId": "9de5d147-27b4-44c5-c265-0889ceda4df9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0, loss: 2.785332441329956\n",
            "epoch: 0, step: 100, loss: 0.19553081691265106\n",
            "epoch: 0, step: 200, loss: 0.1667640507221222\n",
            "epoch: 1, step: 0, loss: 0.1786234974861145\n",
            "epoch: 1, step: 100, loss: 0.11910043656826019\n",
            "epoch: 1, step: 200, loss: 0.14456583559513092\n",
            "epoch: 2, step: 0, loss: 0.115811787545681\n",
            "epoch: 2, step: 100, loss: 0.1400361955165863\n",
            "epoch: 2, step: 200, loss: 0.12958604097366333\n",
            "epoch: 3, step: 0, loss: 0.09612379968166351\n",
            "epoch: 3, step: 100, loss: 0.06782341003417969\n",
            "epoch: 3, step: 200, loss: 0.11805868148803711\n",
            "epoch: 4, step: 0, loss: 0.07453262060880661\n",
            "epoch: 4, step: 100, loss: 0.08563405275344849\n",
            "epoch: 4, step: 200, loss: 0.06428519636392593\n",
            "epoch: 5, step: 0, loss: 0.06816691905260086\n",
            "epoch: 5, step: 100, loss: 0.09515038132667542\n",
            "epoch: 5, step: 200, loss: 0.07228513062000275\n",
            "epoch: 6, step: 0, loss: 0.04631855711340904\n",
            "epoch: 6, step: 100, loss: 0.043122440576553345\n",
            "epoch: 6, step: 200, loss: 0.06853370368480682\n",
            "epoch: 7, step: 0, loss: 0.036460358649492264\n",
            "epoch: 7, step: 100, loss: 0.04376744106411934\n",
            "epoch: 7, step: 200, loss: 0.060541216284036636\n",
            "epoch: 8, step: 0, loss: 0.03403699770569801\n",
            "epoch: 8, step: 100, loss: 0.04559444636106491\n",
            "epoch: 8, step: 200, loss: 0.03924614563584328\n",
            "epoch: 9, step: 0, loss: 0.020921731367707253\n",
            "epoch: 9, step: 100, loss: 0.030193304643034935\n",
            "epoch: 9, step: 200, loss: 0.06258393824100494\n",
            "Время тренировки LSTM = 0:02:08.245586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'I am doing my homework right now'\n",
        "words = phrase.split(' ')\n",
        "tokens = [dataset.word_vocab[w] for w in words]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predict = model(torch.tensor(tokens).unsqueeze(0).to(device)) # 1 x T x N_classes\n",
        "    labels = torch.argmax(predict, dim=-1).squeeze().cpu().detach().tolist()\n",
        "    RNN_inference_time = datetime.datetime.now() - start\n",
        "\n",
        "target_labels = list(dataset.target_vocab.keys())\n",
        "print([target_labels[l] for l in labels])\n",
        "print (f'Время инференса RNN = {RNN_inference_time}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtqdvC7GFyP1",
        "outputId": "246ebc4c-dc7c-4ce8-8692-938d8b16548e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV', 'ADV']\n",
            "Время инференса RNN = 0:00:00.002222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FazAwmhFyel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN with char input"
      ],
      "metadata": {
        "id": "MLzzw5lxOZTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(input_data):\n",
        "    data = []\n",
        "    chars = []\n",
        "    targets = []\n",
        "    max_len = 0\n",
        "    for item in input_data:\n",
        "        if len(item['data']) > max_len:\n",
        "            max_len = len(item['data'])\n",
        "        data.append(torch.as_tensor(item['data']))\n",
        "        chars.append(item['char'])\n",
        "        targets.append(torch.as_tensor(item['target']))\n",
        "    chars_seq = [[torch.as_tensor([0]) for _ in range(len(input_data))] for _ in range(max_len)]\n",
        "    for j in range(len(input_data)):\n",
        "        for i in range(max_len):\n",
        "            if len(chars[j]) > i:\n",
        "                chars_seq[i][j] = torch.as_tensor(chars[j][i])\n",
        "    for j in range(max_len):\n",
        "        chars_seq[j] = pad_sequence(chars_seq[j], batch_first=True, padding_value=0)\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    return {\n",
        "        'data': data, # B x T\n",
        "        'chars': chars_seq, # List[tensor];   tensor B x word_len; len(chars_seq) = n_words =  T\n",
        "        'target': targets}"
      ],
      "metadata": {
        "id": "Dc3Srxg2u1_G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.char_emb(x)\n",
        "        _, out = self.rnn(emb) # 1 x B x Hid\n",
        "\n",
        "        return out.squeeze().unsqueeze(1) # B x 1 x Hid"
      ],
      "metadata": {
        "id": "0XWoHWt02s9X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, n_classes, \n",
        "                 char_vocab, char_emb, char_hidden):\n",
        "        super().__init__()\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        #TODO try to use other RNN archicetures, f.e. RNN and LSTM\n",
        "        self.word_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + char_hidden, hidden_dim, batch_first=True)\n",
        "        self.clf = nn.Linear(hidden_dim, n_classes)\n",
        "        self.do = nn.Dropout(0.1)\n",
        "        self.char_rnn = CharRNN(char_vocab, char_emb, char_hidden)\n",
        "    \n",
        "    def forward(self, x, chars):\n",
        "        emb = self.word_emb(x) # B x T x Emb_dim\n",
        "        char_feat = [self.char_rnn(c.to(x.device)) for c in chars] \n",
        "        char_feat = torch.cat(char_feat, dim=1) # B x T x Hid_char\n",
        "        emb = torch.cat((emb, char_feat), dim=-1)\n",
        "        hidden, _ = self.rnn(emb) # B x T x Hid, B x 1 x Hid\n",
        "        pred = self.clf(self.do(hidden)) # B x T x N_classes\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "4BbmVPBZ2tW_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper params\n",
        "vocab_size = len(dataset.word_vocab) + 1\n",
        "n_classes = len(dataset.target_vocab) + 1\n",
        "n_chars = len(dataset.char_vocab) + 1\n",
        "#TODO try to use other model parameters\n",
        "emb_dim = 256\n",
        "hidden = 256\n",
        "char_hid = 64\n",
        "char_emb = 32\n",
        "n_epochs = 10\n",
        "batch_size = 64\n",
        "cuda_device = 0\n",
        "batch_size = 100\n",
        "device = f'cuda:{cuda_device}' if cuda_device != -1 else 'cpu'"
      ],
      "metadata": {
        "id": "4KWTQbt82ttp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharGRU(vocab_size, emb_dim, hidden, n_classes, n_chars, char_emb, char_hid).to(device)\n",
        "model.train()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FpHMIEDG2uD6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "for epoch in range(n_epochs):\n",
        "    dataloader = DataLoader(dataset, \n",
        "                            batch_size, \n",
        "                            shuffle=True, \n",
        "                            collate_fn=collate_fn,\n",
        "                            drop_last = True,\n",
        "                            )\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        predict = model(batch['data'].to(device), batch['chars'])\n",
        "        loss = loss_func(predict.view(-1, n_classes),\n",
        "                         batch['target'].to(device).view(-1), \n",
        "                         )\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if i % 100 == 0:\n",
        "            print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
        "\n",
        "    CharGRU_train_time = datetime.datetime.now() - start\n",
        "   \n",
        "    torch.save(model.state_dict(), f'./rnn_chkpt_{epoch}.pth')\n",
        "print (f'Время тренировки GRU на буквах = {CharGRU_train_time}' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrBflUxA8ygA",
        "outputId": "06e3512e-207a-48bc-e2d6-4499f101359f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, step: 0, loss: 0.10503656417131424\n",
            "epoch: 0, step: 100, loss: 0.07406461983919144\n",
            "epoch: 0, step: 200, loss: 0.07979340106248856\n",
            "epoch: 1, step: 0, loss: 0.05045900121331215\n",
            "epoch: 1, step: 100, loss: 0.07383377104997635\n",
            "epoch: 1, step: 200, loss: 0.043683238327503204\n",
            "epoch: 2, step: 0, loss: 0.06219891086220741\n",
            "epoch: 2, step: 100, loss: 0.06995214521884918\n",
            "epoch: 2, step: 200, loss: 0.09226678311824799\n",
            "epoch: 3, step: 0, loss: 0.04853523150086403\n",
            "epoch: 3, step: 100, loss: 0.041543953120708466\n",
            "epoch: 3, step: 200, loss: 0.046959348022937775\n",
            "epoch: 4, step: 0, loss: 0.01868255063891411\n",
            "epoch: 4, step: 100, loss: 0.038568202406167984\n",
            "epoch: 4, step: 200, loss: 0.04018765687942505\n",
            "epoch: 5, step: 0, loss: 0.030659547075629234\n",
            "epoch: 5, step: 100, loss: 0.04228643327951431\n",
            "epoch: 5, step: 200, loss: 0.019917916506528854\n",
            "epoch: 6, step: 0, loss: 0.0328444167971611\n",
            "epoch: 6, step: 100, loss: 0.03597190976142883\n",
            "epoch: 6, step: 200, loss: 0.04313971474766731\n",
            "epoch: 7, step: 0, loss: 0.03150536119937897\n",
            "epoch: 7, step: 100, loss: 0.02495349384844303\n",
            "epoch: 7, step: 200, loss: 0.033335983753204346\n",
            "epoch: 8, step: 0, loss: 0.010726271197199821\n",
            "epoch: 8, step: 100, loss: 0.0309365913271904\n",
            "epoch: 8, step: 200, loss: 0.0256764218211174\n",
            "epoch: 9, step: 0, loss: 0.021917391568422318\n",
            "epoch: 9, step: 100, loss: 0.01709182932972908\n",
            "epoch: 9, step: 200, loss: 0.01706349104642868\n",
            "Время тренировки GRU на буквах = 0:03:52.695593\n"
          ]
        }
      ]
    }
  ]
}